{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nbformat\n",
    "from nbformat.v4 import new_code_cell, new_markdown_cell, new_raw_cell\n",
    "from nbconvert import HTMLExporter\n",
    "from nbconvert.preprocessors import ExecutePreprocessor, CellExecutionError\n",
    "import re\n",
    "import textwrap\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = glob.glob('../../accy575-sp-2023/02-pcard-submissions/*.ipynb')\n",
    "fpaths = list(filter(lambda f: not f.endswith('-graded.ipynb'), fpaths))\n",
    "fpaths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../submissions/exercise-05\\\\chenchloe_83497_7409117_exercise_05_pandas_filtering_sorting.ipynb',\n",
       " '../submissions/exercise-05\\\\esparzajoel_51499_7451315_Copy_of_exercise_05_pandas_filtering_sorting.ipynb',\n",
       " '../submissions/exercise-05\\\\tabornatalie_8175_7404582_exercise_05_pandas_filtering_sorting.ipynb']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpaths = glob.glob('../submissions/exercise-05/*.ipynb')\n",
    "fpaths = list(filter(lambda f: not f.endswith('-graded.ipynb'), fpaths))\n",
    "fpaths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "Running ../submissions/exercise-05\\chenchloe_83497_7409117_exercise_05_pandas_filtering_sorting.ipynb successful\n",
      "Stored graded result as JSON to ../submissions/exercise-05\\chenchloe_83497_7409117_exercise_05_pandas_filtering_sorting-result.json\n",
      "{'filename': 'chenchloe_83497_7409117_exercise_05_pandas_filtering_sorting-graded.ipynb', 'grading_finished_at': '2023-02-18T05:17:57.014983', 'grading_duration_in_seconds': 0.85, 'learner_score': 20, 'total_available': 20, 'num_test_cases': 8, 'num_passed_cases': 8, 'num_failed_cases': 0, 'summary': 'File: chenchloe_83497_7409117_exercise_05_pandas_filtering_sorting-graded.ipynb\\nScore: 20 out of 20\\nPassed 8 out of 8 test cases\\nGrading took 0.85 seconds\\n\\nTest Case Summary\\n===\\nimport-pandas-numpy passed: 2 out of 2 points\\n===\\ncreate-a-pandas-series passed: 2 out of 2 points\\n===\\ncreate-a-pandas-dataframe passed: 2 out of 2 points\\n===\\nfind-num-rows-and-cols passed: 2 out of 2 points\\n===\\nfind-all-games passed: 3 out of 3 points\\n===\\nfind-electronics-over-10-dollars passed: 4 out of 4 points\\n===\\nsort-by-price-desc passed: 2 out of 2 points\\n===\\nsort-by-cat-asc-name-desc passed: 3 out of 3 points\\n'}\n",
      "=============================\n",
      "Running ../submissions/exercise-05\\esparzajoel_51499_7451315_Copy_of_exercise_05_pandas_filtering_sorting.ipynb successful\n",
      "Stored graded result as JSON to ../submissions/exercise-05\\esparzajoel_51499_7451315_Copy_of_exercise_05_pandas_filtering_sorting-result.json\n",
      "{'filename': 'esparzajoel_51499_7451315_Copy_of_exercise_05_pandas_filtering_sorting-graded.ipynb', 'grading_finished_at': '2023-02-18T05:18:00.166108', 'grading_duration_in_seconds': 0.82, 'learner_score': 17, 'total_available': 20, 'num_test_cases': 8, 'num_passed_cases': 7, 'num_failed_cases': 1, 'summary': \"File: esparzajoel_51499_7451315_Copy_of_exercise_05_pandas_filtering_sorting-graded.ipynb\\nScore: 17 out of 20\\nPassed 7 out of 8 test cases\\nGrading took 0.82 seconds\\n\\nTest Case Summary\\n===\\nimport-pandas-numpy passed: 2 out of 2 points\\n===\\ncreate-a-pandas-series passed: 2 out of 2 points\\n===\\ncreate-a-pandas-dataframe passed: 2 out of 2 points\\n===\\nfind-num-rows-and-cols passed: 2 out of 2 points\\n===\\nfind-all-games passed: 3 out of 3 points\\n===\\nfind-electronics-over-10-dollars passed: 4 out of 4 points\\n===\\nsort-by-price-desc passed: 2 out of 2 points\\n===\\nsort-by-cat-asc-name-desc failed: 0 out of 3 points\\n[Autograder Output]\\nNameError: name 'df_sorted_by_category_price' is not defined\\n\\n\"}\n",
      "=============================\n",
      "Running ../submissions/exercise-05\\tabornatalie_8175_7404582_exercise_05_pandas_filtering_sorting.ipynb successful\n",
      "Stored graded result as JSON to ../submissions/exercise-05\\tabornatalie_8175_7404582_exercise_05_pandas_filtering_sorting-result.json\n",
      "{'filename': 'tabornatalie_8175_7404582_exercise_05_pandas_filtering_sorting-graded.ipynb', 'grading_finished_at': '2023-02-18T05:18:03.340275', 'grading_duration_in_seconds': 0.8, 'learner_score': 10, 'total_available': 20, 'num_test_cases': 8, 'num_passed_cases': 5, 'num_failed_cases': 3, 'summary': \"File: tabornatalie_8175_7404582_exercise_05_pandas_filtering_sorting-graded.ipynb\\nScore: 10 out of 20\\nPassed 5 out of 8 test cases\\nGrading took 0.8 seconds\\n\\nTest Case Summary\\n===\\nimport-pandas-numpy passed: 2 out of 2 points\\n===\\ncreate-a-pandas-series passed: 2 out of 2 points\\n===\\ncreate-a-pandas-dataframe passed: 2 out of 2 points\\n===\\nfind-num-rows-and-cols passed: 2 out of 2 points\\n===\\nfind-all-games failed: 0 out of 3 points\\n[Autograder Output]\\nAttributeError: 'Series' object has no attribute 'columns'\\n\\n===\\nfind-electronics-over-10-dollars failed: 0 out of 4 points\\n[Autograder Output]\\nAttributeError: 'Series' object has no attribute 'columns'\\n\\n===\\nsort-by-price-desc passed: 2 out of 2 points\\n===\\nsort-by-cat-asc-name-desc failed: 0 out of 3 points\\n[Autograder Output]\\nNameError: name 'df_sorted_by_category_price' is not defined\\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "from lambdagrader import *\n",
    "\n",
    "graded_results = []\n",
    "\n",
    "for notebook_path in fpaths:\n",
    "    try:\n",
    "        print('=============================')\n",
    "        nb = nbformat.read(notebook_path, as_version=4)\n",
    "        \n",
    "        for cell in nb.cells:\n",
    "            test_case_metadata = extract_test_case_metadata_from_cell(cell.source)\n",
    "\n",
    "            if test_case_metadata:\n",
    "                cell.source = convert_to_grader_code(cell.source)\n",
    "\n",
    "        add_scripts_to_notebook(nb)\n",
    "        \n",
    "        ep = ExecutePreprocessor(\n",
    "            timeout=1800,\n",
    "            kernel_name='python3',\n",
    "            allow_errors=True\n",
    "        )\n",
    "        ep.preprocess(nb)\n",
    "        print(f'Running {notebook_path} successful')\n",
    "        \n",
    "        # save graded notebook\n",
    "        converted_notebook_path = notebook_path.replace('.ipynb', '-graded.ipynb')\n",
    "        with open(converted_notebook_path, mode='w', encoding='utf-8') as f:\n",
    "            nbformat.write(nb, f)\n",
    "        \n",
    "        # rename graded result JSON file\n",
    "        graded_result_json_path = notebook_path.replace('.ipynb', '-result.json')\n",
    "        shutil.move('lambdagrader-result.json', graded_result_json_path)\n",
    "        print(f'Stored graded result as JSON to {graded_result_json_path}')\n",
    "        \n",
    "        # read graded result to generate a summary\n",
    "        with open(graded_result_json_path, mode='r') as f:\n",
    "            graded_result = json.load(f)\n",
    "            \n",
    "        graded_notebook_filename = Path(converted_notebook_path).name\n",
    "            \n",
    "        summary = ''\n",
    "        summary += f\"File: {graded_notebook_filename}\\n\"\n",
    "        summary += f\"Score: {graded_result['learner_score']} out of {graded_result['total_available']}\\n\"\n",
    "        summary += f\"Passed {graded_result['num_passed_cases']} out of {graded_result['num_test_cases']} test cases\\n\"\n",
    "        summary += f\"Grading took {graded_result['grading_duration_in_seconds']} seconds\\n\\n\"\n",
    "        summary += 'Test Case Summary\\n'\n",
    "        \n",
    "        for o in graded_result['results']:\n",
    "            summary += \"===\\n\"\n",
    "            summary += f\"{o['test_case_name']} {'passed' if o['pass'] else 'failed'}: {o['points']} out of {o['available_points']} points\\n\"\n",
    "            \n",
    "            if not o['pass']:\n",
    "                summary += f\"[Autograder Output]\\n{o['message']}\\n\\n\"\n",
    "                \n",
    "        result_summary = {\n",
    "            'filename': graded_notebook_filename,\n",
    "            'grading_finished_at': graded_result['grading_finished_at'],\n",
    "            'grading_duration_in_seconds': graded_result['grading_duration_in_seconds'],\n",
    "            'learner_score': graded_result['learner_score'],\n",
    "            'total_available': graded_result['total_available'],\n",
    "            'num_test_cases': graded_result['num_test_cases'],\n",
    "            'num_passed_cases': graded_result['num_passed_cases'],\n",
    "            'num_failed_cases': graded_result['num_failed_cases'],\n",
    "            'summary': summary\n",
    "        }\n",
    "\n",
    "        # remove prepend, append cells added by LambdaGrader before storing to HTML\n",
    "        nb.cells.pop(0)  # first cell (added by LambdaGrader)\n",
    "        nb.cells.pop()   # last cell (added by LambdaGrader)\n",
    "        \n",
    "        insert_index = 0\n",
    "        \n",
    "        # add result summary\n",
    "        nb.cells.insert(insert_index, new_markdown_cell('# 🧭 LambdaGrader Summary'))\n",
    "        insert_index += 1\n",
    "        \n",
    "        nb.cells.insert(insert_index, new_markdown_cell('## Metadata'))\n",
    "        insert_index += 1\n",
    "        \n",
    "        df_metadata = pd.DataFrame({\n",
    "            'name': [\n",
    "                'graded_filename',\n",
    "                'grading_finished_at',\n",
    "                'grading_duration',\n",
    "                '**learner_score**',\n",
    "                'max_score',\n",
    "                'learner_score_in_percentage',\n",
    "                'num_test_cases',\n",
    "                'num_passed_cases',\n",
    "                'num_failed_cases'\n",
    "            ],\n",
    "            'value': [\n",
    "                graded_notebook_filename,\n",
    "                graded_result['grading_finished_at'],\n",
    "                f\"{graded_result['grading_duration_in_seconds']} second{'' if graded_result['grading_duration_in_seconds'] == 0 else 's'}\",\n",
    "                f\"**{graded_result['learner_score']}**\",\n",
    "                graded_result['total_available'],\n",
    "                f\"{round(graded_result['learner_score'] / graded_result['total_available'] * 100, 2)}%\",\n",
    "                graded_result['num_test_cases'],\n",
    "                graded_result['num_passed_cases'],\n",
    "                graded_result['num_failed_cases']\n",
    "            ]\n",
    "        })\n",
    "        nb.cells.insert(insert_index, new_markdown_cell(df_metadata.to_markdown(index=False)))\n",
    "        insert_index += 1\n",
    "        \n",
    "        nb.cells.insert(insert_index, new_markdown_cell('## Test case results'))\n",
    "        insert_index += 1\n",
    "        \n",
    "        df_r = pd.DataFrame(graded_result['results'])\n",
    "        df_r['pass'] = df_r['pass'].map({\n",
    "            True: '✔️ Pass', False: '❌ Fail'\n",
    "        })\n",
    "        # \n",
    "        df_r.rename(columns={\n",
    "            'available_points': 'max_score',\n",
    "            'points': 'learner_score',\n",
    "            'pass': 'result'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        nb.cells.insert(insert_index, new_markdown_cell(df_r.to_markdown()))\n",
    "        insert_index += 1\n",
    "        \n",
    "        nb.cells.insert(insert_index, new_markdown_cell('\\n---\\n'))\n",
    "        insert_index += 1\n",
    "        \n",
    "        # store graded result to HTML\n",
    "        graded_html_path = notebook_path.replace('.ipynb', '-graded.html')\n",
    "        html_exporter = HTMLExporter()\n",
    "        r = html_exporter.from_notebook_node(nb)\n",
    "        with open(graded_html_path, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(r[0])\n",
    "        \n",
    "        print(result_summary)\n",
    "        graded_results.append(result_summary)\n",
    "    except CellExecutionError as e:\n",
    "        print(f'CellExecutionError on {notebook_path}')\n",
    "        print('-----------------------------')\n",
    "        print(e)\n",
    "        \n",
    "df_summary = pd.DataFrame(graded_results)\n",
    "\n",
    "df_summary.to_csv(\n",
    "    f\"graded_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n",
    "    index=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambdagrader import *\n",
    "\n",
    "tcs = extract_test_cases_metadata_from_notebook( '../notebooks\\\\case-study-04-rideshare-trips-SOLUTION.ipynb')\n",
    "\n",
    "s = 0\n",
    "\n",
    "for o in tcs:\n",
    "    s += o['points']\n",
    "    \n",
    "print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
